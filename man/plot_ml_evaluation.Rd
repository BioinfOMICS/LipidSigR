% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plot_ml_evaluation.R
\name{plot_ml_evaluation}
\alias{plot_ml_evaluation}
\title{plot_ml_evaluation}
\usage{
plot_ml_evaluation(ml_se, eval_method)
}
\arguments{
\item{ml_se}{A SummarizedExperiment object with results computed by \code{\link{ml_model}}.}

\item{eval_method}{Character. The evaluation method to be used. Allowed methods
include 'Accuracy', 'Sensitivity', 'Specificity', 'Pos Pred Value',
'Neg Pred Value', 'Precision', 'Recall', 'F1', 'Prevalence', 'Detection Rate',
'Detection Prevalence', 'Balanced Accuracy'. Default is \code{'Accuracy'}.}
}
\value{
Return 1 interactive plot, 1 static plot, and 2 tables.
\enumerate{
\item interactive_evaluation_plot & static_evaluation_plot: model performance plot
\item table_evaluation: table of model evaluation information.
\item table_evaluation_plot: table for plotting evaluation plots.
}
}
\description{
This function plots critical indicators to help users evaluate
model performance, including the average values and 95\% confidence intervals
for metrics such as accuracy, sensitivity (recall), specificity, positive
predictive value (precision), negative predictive value, F1 score, prevalence,
detection rate, detection prevalence, and balanced accuracy.
}
\examples{
data("ml_se_sub")
res <- plot_ml_evaluation(ml_se_sub, eval_method='Accuracy')
}
